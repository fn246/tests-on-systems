{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version torch: 1.5.1 \n",
      "librosa version: 0.8.0 \n",
      "mir_eval version: 0.6 \n",
      "visdom version: 0.1.8.9\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import visdom\n",
    "import mir_eval\n",
    "print('version torch:',torch.__version__ , '\\nlibrosa version:',librosa.__version__ , '\\nmir_eval version:',mir_eval.__version__ ,'\\nvisdom version:' ,visdom.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/speech/f_torch/bin\r\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "\n",
    "tr_path='f_torch/codes/mydata/mydata/tr'\n",
    "cv_path='f_torch/codes/mydata/mydata/cv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess.py\n",
    "#!/usr/bin/env python\n",
    "# Created on 2018/12\n",
    "# Author: Kaituo XU\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "\n",
    "\n",
    "def preprocess_one_dir(in_dir, out_dir, out_filename, sample_rate=8000):\n",
    "    file_infos = []\n",
    "    in_dir = os.path.abspath(in_dir)\n",
    "    wav_list = os.listdir(in_dir)\n",
    "    for wav_file in wav_list:\n",
    "        if not wav_file.endswith('.wav'):\n",
    "            continue\n",
    "        wav_path = os.path.join(in_dir, wav_file)\n",
    "        samples, _ = librosa.load(wav_path, sr=sample_rate)\n",
    "        file_infos.append((wav_path, len(samples)))\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    with open(os.path.join(out_dir, out_filename + '.json'), 'w') as f:\n",
    "        json.dump(file_infos, f, indent=4)\n",
    "\n",
    "\n",
    "def preprocess(args):\n",
    "    for data_type in ['tr', 'cv', 'tt']:\n",
    "        for speaker in ['mix', 's1', 's2']:\n",
    "            preprocess_one_dir(os.path.join(args.in_dir, data_type, speaker),\n",
    "                               os.path.join(args.out_dir, data_type),\n",
    "                               speaker,\n",
    "                               sample_rate=args.sample_rate)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(\"WSJ0 data preprocessing\")\n",
    "#     parser.add_argument('--in-dir', type=str, default=None,\n",
    "#                         help='Directory path of wsj0 including tr, cv and tt')\n",
    "#     parser.add_argument('--out-dir', type=str, default=None,\n",
    "#                         help='Directory path to put output files')\n",
    "#     parser.add_argument('--sample-rate', type=int, default=8000,\n",
    "#                         help='Sample rate of audio file')\n",
    "#     args = parser.parse_args()\n",
    "#     print(args)\n",
    "#     preprocess(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.py\n",
    "# Created on 2018/12\n",
    "# Author: Kaituo XU\n",
    "\"\"\"\n",
    "Logic:\n",
    "1. AudioDataLoader generate a minibatch from AudioDataset, the size of this\n",
    "   minibatch is AudioDataLoader's batchsize. For now, we always set\n",
    "   AudioDataLoader's batchsize as 1. The real minibatch size we care about is\n",
    "   set in AudioDataset's __init__(...). So actually, we generate the\n",
    "   information of one minibatch in AudioDataset.\n",
    "2. After AudioDataLoader getting one minibatch from AudioDataset,\n",
    "   AudioDataLoader calls its collate_fn(batch) to process this minibatch.\n",
    "Input:\n",
    "    Mixtured WJS0 tr, cv and tt path\n",
    "Output:\n",
    "    One batch at a time.\n",
    "    Each inputs's shape is B x T\n",
    "    Each targets's shape is B x C x T\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "import librosa\n",
    "\n",
    "\n",
    "class AudioDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, json_dir, batch_size, sample_rate=8000, segment=4.0, cv_maxlen=8.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            json_dir: directory including mix.json, s1.json and s2.json\n",
    "            segment: duration of audio segment, when set to -1, use full audio\n",
    "        xxx_infos is a list and each item is a tuple (wav_file, #samples)\n",
    "        \"\"\"\n",
    "        super(AudioDataset, self).__init__()\n",
    "        mix_json = os.path.join(json_dir, 'mix.json')\n",
    "        s1_json = os.path.join(json_dir, 's1.json')\n",
    "        s2_json = os.path.join(json_dir, 's2.json')\n",
    "        with open(mix_json, 'r') as f:\n",
    "            mix_infos = json.load(f)\n",
    "        with open(s1_json, 'r') as f:\n",
    "            s1_infos = json.load(f)\n",
    "        with open(s2_json, 'r') as f:\n",
    "            s2_infos = json.load(f)\n",
    "        # sort it by #samples (impl bucket)\n",
    "        def sort(infos): return sorted(\n",
    "            infos, key=lambda info: int(info[1]), reverse=True)\n",
    "        sorted_mix_infos = sort(mix_infos)\n",
    "        sorted_s1_infos = sort(s1_infos)\n",
    "        sorted_s2_infos = sort(s2_infos)\n",
    "        if segment >= 0.0:\n",
    "            # segment length and count dropped utts\n",
    "            segment_len = int(segment * sample_rate)  # 4s * 8000/s = 32000 samples\n",
    "            drop_utt, drop_len = 0, 0\n",
    "            for _, sample in sorted_mix_infos:\n",
    "                if sample < segment_len:\n",
    "                    drop_utt += 1\n",
    "                    drop_len += sample\n",
    "            print(\"Drop {} utts({:.2f} h) which is short than {} samples\".format(\n",
    "                drop_utt, drop_len/sample_rate/36000, segment_len))\n",
    "            # generate minibach infomations\n",
    "            minibatch = []\n",
    "            start = 0\n",
    "            while True:\n",
    "                num_segments = 0\n",
    "                end = start\n",
    "                part_mix, part_s1, part_s2 = [], [], []\n",
    "                while num_segments < batch_size and end < len(sorted_mix_infos):\n",
    "                    utt_len = int(sorted_mix_infos[end][1])\n",
    "                    if utt_len >= segment_len:  # skip too short utt\n",
    "                        num_segments += math.ceil(utt_len / segment_len)\n",
    "                        # Ensure num_segments is less than batch_size\n",
    "                        if num_segments > batch_size:\n",
    "                            # if num_segments of 1st audio > batch_size, skip it\n",
    "                            if start == end: end += 1\n",
    "                            break\n",
    "                        part_mix.append(sorted_mix_infos[end])\n",
    "                        part_s1.append(sorted_s1_infos[end])\n",
    "                        part_s2.append(sorted_s2_infos[end])\n",
    "                    end += 1\n",
    "                if len(part_mix) > 0:\n",
    "                    minibatch.append([part_mix, part_s1, part_s2,\n",
    "                                      sample_rate, segment_len])\n",
    "                if end == len(sorted_mix_infos):\n",
    "                    break\n",
    "                start = end\n",
    "            self.minibatch = minibatch\n",
    "        else:  # Load full utterance but not segment\n",
    "            # generate minibach infomations\n",
    "            minibatch = []\n",
    "            start = 0\n",
    "            while True:\n",
    "                end = min(len(sorted_mix_infos), start + batch_size)\n",
    "                # Skip long audio to avoid out-of-memory issue\n",
    "                if int(sorted_mix_infos[start][1]) > cv_maxlen * sample_rate:\n",
    "                    start = end\n",
    "                    continue\n",
    "                minibatch.append([sorted_mix_infos[start:end],\n",
    "                                  sorted_s1_infos[start:end],\n",
    "                                  sorted_s2_infos[start:end],\n",
    "                                  sample_rate, segment])\n",
    "                if end == len(sorted_mix_infos):\n",
    "                    break\n",
    "                start = end\n",
    "            self.minibatch = minibatch\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.minibatch[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.minibatch)\n",
    "\n",
    "\n",
    "class AudioDataLoader(data.DataLoader):\n",
    "    \"\"\"\n",
    "    NOTE: just use batchsize=1 here, so drop_last=True makes no sense here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AudioDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = _collate_fn\n",
    "\n",
    "\n",
    "def _collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        batch: list, len(batch) = 1. See AudioDataset.__getitem__()\n",
    "    Returns:\n",
    "        mixtures_pad: B x T, torch.Tensor\n",
    "        ilens : B, torch.Tentor\n",
    "        sources_pad: B x C x T, torch.Tensor\n",
    "    \"\"\"\n",
    "    # batch should be located in list\n",
    "    assert len(batch) == 1\n",
    "    mixtures, sources = load_mixtures_and_sources(batch[0])\n",
    "\n",
    "    # get batch of lengths of input sequences\n",
    "    ilens = np.array([mix.shape[0] for mix in mixtures])\n",
    "\n",
    "    # perform padding and convert to tensor\n",
    "    pad_value = 0\n",
    "    mixtures_pad = pad_list([torch.from_numpy(mix).float()\n",
    "                             for mix in mixtures], pad_value)\n",
    "    ilens = torch.from_numpy(ilens)\n",
    "    sources_pad = pad_list([torch.from_numpy(s).float()\n",
    "                            for s in sources], pad_value)\n",
    "    # N x T x C -> N x C x T\n",
    "    sources_pad = sources_pad.permute((0, 2, 1)).contiguous()\n",
    "    return mixtures_pad, ilens, sources_pad\n",
    "\n",
    "\n",
    "# Eval data part\n",
    "# from preprocess import preprocess_one_dir\n",
    "\n",
    "\n",
    "class EvalDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, mix_dir, mix_json, batch_size, sample_rate=8000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mix_dir: directory including mixture wav files\n",
    "            mix_json: json file including mixture wav files\n",
    "        \"\"\"\n",
    "        super(EvalDataset, self).__init__()\n",
    "        assert mix_dir != None or mix_json != None\n",
    "        if mix_dir is not None:\n",
    "            # Generate mix.json given mix_dir\n",
    "            preprocess_one_dir(mix_dir, mix_dir, 'mix',\n",
    "                               sample_rate=sample_rate)\n",
    "            mix_json = os.path.join(mix_dir, 'mix.json')\n",
    "        with open(mix_json, 'r') as f:\n",
    "            mix_infos = json.load(f)\n",
    "        # sort it by #samples (impl bucket)\n",
    "        def sort(infos): return sorted(\n",
    "            infos, key=lambda info: int(info[1]), reverse=True)\n",
    "        sorted_mix_infos = sort(mix_infos)\n",
    "        # generate minibach infomations\n",
    "        minibatch = []\n",
    "        start = 0\n",
    "        while True:\n",
    "            end = min(len(sorted_mix_infos), start + batch_size)\n",
    "            minibatch.append([sorted_mix_infos[start:end],\n",
    "                              sample_rate])\n",
    "            if end == len(sorted_mix_infos):\n",
    "                break\n",
    "            start = end\n",
    "        self.minibatch = minibatch\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.minibatch[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.minibatch)\n",
    "\n",
    "\n",
    "class EvalDataLoader(data.DataLoader):\n",
    "    \"\"\"\n",
    "    NOTE: just use batchsize=1 here, so drop_last=True makes no sense here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(EvalDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = _collate_fn_eval\n",
    "\n",
    "\n",
    "def _collate_fn_eval(batch):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        batch: list, len(batch) = 1. See AudioDataset.__getitem__()\n",
    "    Returns:\n",
    "        mixtures_pad: B x T, torch.Tensor\n",
    "        ilens : B, torch.Tentor\n",
    "        filenames: a list contain B strings\n",
    "    \"\"\"\n",
    "    # batch should be located in list\n",
    "    assert len(batch) == 1\n",
    "    mixtures, filenames = load_mixtures(batch[0])\n",
    "\n",
    "    # get batch of lengths of input sequences\n",
    "    ilens = np.array([mix.shape[0] for mix in mixtures])\n",
    "\n",
    "    # perform padding and convert to tensor\n",
    "    pad_value = 0\n",
    "    mixtures_pad = pad_list([torch.from_numpy(mix).float()\n",
    "                             for mix in mixtures], pad_value)\n",
    "    ilens = torch.from_numpy(ilens)\n",
    "    return mixtures_pad, ilens, filenames\n",
    "\n",
    "\n",
    "# ------------------------------ utils ------------------------------------\n",
    "def load_mixtures_and_sources(batch):\n",
    "    \"\"\"\n",
    "    Each info include wav path and wav duration.\n",
    "    Returns:\n",
    "        mixtures: a list containing B items, each item is T np.ndarray\n",
    "        sources: a list containing B items, each item is T x C np.ndarray\n",
    "        T varies from item to item.\n",
    "    \"\"\"\n",
    "    mixtures, sources = [], []\n",
    "    mix_infos, s1_infos, s2_infos, sample_rate, segment_len = batch\n",
    "    # for each utterance\n",
    "    for mix_info, s1_info, s2_info in zip(mix_infos, s1_infos, s2_infos):\n",
    "        mix_path = mix_info[0]\n",
    "        s1_path = s1_info[0]\n",
    "        s2_path = s2_info[0]\n",
    "        assert mix_info[1] == s1_info[1] and s1_info[1] == s2_info[1]\n",
    "        # read wav file\n",
    "        mix, _ = librosa.load(mix_path, sr=sample_rate)\n",
    "        s1, _ = librosa.load(s1_path, sr=sample_rate)\n",
    "        s2, _ = librosa.load(s2_path, sr=sample_rate)\n",
    "        # merge s1 and s2\n",
    "        s = np.dstack((s1, s2))[0]  # T x C, C = 2\n",
    "        utt_len = mix.shape[-1]\n",
    "        if segment_len >= 0:\n",
    "            # segment\n",
    "            for i in range(0, utt_len - segment_len + 1, segment_len):\n",
    "                mixtures.append(mix[i:i+segment_len])\n",
    "                sources.append(s[i:i+segment_len])\n",
    "            if utt_len % segment_len != 0:\n",
    "                mixtures.append(mix[-segment_len:])\n",
    "                sources.append(s[-segment_len:])\n",
    "        else:  # full utterance\n",
    "            mixtures.append(mix)\n",
    "            sources.append(s)\n",
    "    return mixtures, sources\n",
    "\n",
    "\n",
    "def load_mixtures(batch):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        mixtures: a list containing B items, each item is T np.ndarray\n",
    "        filenames: a list containing B strings\n",
    "        T varies from item to item.\n",
    "    \"\"\"\n",
    "    mixtures, filenames = [], []\n",
    "    mix_infos, sample_rate = batch\n",
    "    # for each utterance\n",
    "    for mix_info in mix_infos:\n",
    "        mix_path = mix_info[0]\n",
    "        # read wav file\n",
    "        mix, _ = librosa.load(mix_path, sr=sample_rate)\n",
    "        mixtures.append(mix)\n",
    "        filenames.append(mix_path)\n",
    "    return mixtures, filenames\n",
    "\n",
    "\n",
    "def pad_list(xs, pad_value):\n",
    "    n_batch = len(xs)\n",
    "    max_len = max(x.size(0) for x in xs)\n",
    "    pad = xs[0].new(n_batch, max_len, * xs[0].size()[1:]).fill_(pad_value)\n",
    "    for i in range(n_batch):\n",
    "        pad[i, :xs[i].size(0)] = xs[i]\n",
    "    return pad\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import sys\n",
    "#     json_dir, batch_size = sys.argv[1:3]\n",
    "#     dataset = AudioDataset(json_dir, int(batch_size))\n",
    "#     data_loader = AudioDataLoader(dataset, batch_size=1,\n",
    "#                                   num_workers=4)\n",
    "#     for i, batch in enumerate(data_loader):\n",
    "#         mixtures, lens, sources = batch\n",
    "#         print(i)\n",
    "#         print(mixtures.size())\n",
    "#         print(sources.size())\n",
    "#         print(lens)\n",
    "#         if i < 10:\n",
    "#             print(mixtures)\n",
    "#             print(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pit_criterion.py\n",
    "# Created on 2018/12\n",
    "# Author: Kaituo XU\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "\n",
    "def cal_loss(source, estimate_source, source_lengths):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: [B, C, T], B is batch size\n",
    "        estimate_source: [B, C, T]\n",
    "        source_lengths: [B]\n",
    "    \"\"\"\n",
    "    max_snr, perms, max_snr_idx = cal_si_snr_with_pit(source,\n",
    "                                                      estimate_source,\n",
    "                                                      source_lengths)\n",
    "    loss = 0 - torch.mean(max_snr)\n",
    "    reorder_estimate_source = reorder_source(estimate_source, perms, max_snr_idx)\n",
    "    return loss, max_snr, estimate_source, reorder_estimate_source\n",
    "\n",
    "\n",
    "def cal_si_snr_with_pit(source, estimate_source, source_lengths):\n",
    "    \"\"\"Calculate SI-SNR with PIT training.\n",
    "    Args:\n",
    "        source: [B, C, T], B is batch size\n",
    "        estimate_source: [B, C, T]\n",
    "        source_lengths: [B], each item is between [0, T]\n",
    "    \"\"\"\n",
    "    assert source.size() == estimate_source.size()\n",
    "    B, C, T = source.size()\n",
    "    # mask padding position along T\n",
    "    mask = get_mask(source, source_lengths)\n",
    "    estimate_source *= mask\n",
    "\n",
    "    # Step 1. Zero-mean norm\n",
    "    num_samples = source_lengths.view(-1, 1, 1).float()  # [B, 1, 1]\n",
    "    mean_target = torch.sum(source, dim=2, keepdim=True) / num_samples\n",
    "    mean_estimate = torch.sum(estimate_source, dim=2, keepdim=True) / num_samples\n",
    "    zero_mean_target = source - mean_target\n",
    "    zero_mean_estimate = estimate_source - mean_estimate\n",
    "    # mask padding position along T\n",
    "    zero_mean_target *= mask\n",
    "    zero_mean_estimate *= mask\n",
    "\n",
    "    # Step 2. SI-SNR with PIT\n",
    "    # reshape to use broadcast\n",
    "    s_target = torch.unsqueeze(zero_mean_target, dim=1)  # [B, 1, C, T]\n",
    "    s_estimate = torch.unsqueeze(zero_mean_estimate, dim=2)  # [B, C, 1, T]\n",
    "    # s_target = <s', s>s / ||s||^2\n",
    "    pair_wise_dot = torch.sum(s_estimate * s_target, dim=3, keepdim=True)  # [B, C, C, 1]\n",
    "    s_target_energy = torch.sum(s_target ** 2, dim=3, keepdim=True) + EPS  # [B, 1, C, 1]\n",
    "    pair_wise_proj = pair_wise_dot * s_target / s_target_energy  # [B, C, C, T]\n",
    "    # e_noise = s' - s_target\n",
    "    e_noise = s_estimate - pair_wise_proj  # [B, C, C, T]\n",
    "    # SI-SNR = 10 * log_10(||s_target||^2 / ||e_noise||^2)\n",
    "    pair_wise_si_snr = torch.sum(pair_wise_proj ** 2, dim=3) / (torch.sum(e_noise ** 2, dim=3) + EPS)\n",
    "    pair_wise_si_snr = 10 * torch.log10(pair_wise_si_snr + EPS)  # [B, C, C]\n",
    "\n",
    "    # Get max_snr of each utterance\n",
    "    # permutations, [C!, C]\n",
    "    perms = source.new_tensor(list(permutations(range(C))), dtype=torch.long)\n",
    "    # one-hot, [C!, C, C]\n",
    "    index = torch.unsqueeze(perms, 2)\n",
    "    perms_one_hot = source.new_zeros((*perms.size(), C)).scatter_(2, index, 1)\n",
    "    # [B, C!] <- [B, C, C] einsum [C!, C, C], SI-SNR sum of each permutation\n",
    "    snr_set = torch.einsum('bij,pij->bp', [pair_wise_si_snr, perms_one_hot])\n",
    "    max_snr_idx = torch.argmax(snr_set, dim=1)  # [B]\n",
    "    # max_snr = torch.gather(snr_set, 1, max_snr_idx.view(-1, 1))  # [B, 1]\n",
    "    max_snr, _ = torch.max(snr_set, dim=1, keepdim=True)\n",
    "    max_snr /= C\n",
    "    return max_snr, perms, max_snr_idx\n",
    "\n",
    "\n",
    "def reorder_source(source, perms, max_snr_idx):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: [B, C, T]\n",
    "        perms: [C!, C], permutations\n",
    "        max_snr_idx: [B], each item is between [0, C!)\n",
    "    Returns:\n",
    "        reorder_source: [B, C, T]\n",
    "    \"\"\"\n",
    "    B, C, *_ = source.size()\n",
    "    # [B, C], permutation whose SI-SNR is max of each utterance\n",
    "    # for each utterance, reorder estimate source according this permutation\n",
    "    max_snr_perm = torch.index_select(perms, dim=0, index=max_snr_idx)\n",
    "    # print('max_snr_perm', max_snr_perm)\n",
    "    # maybe use torch.gather()/index_select()/scatter() to impl this?\n",
    "    reorder_source = torch.zeros_like(source)\n",
    "    for b in range(B):\n",
    "        for c in range(C):\n",
    "            reorder_source[b, c] = source[b, max_snr_perm[b][c]]\n",
    "    return reorder_source\n",
    "\n",
    "\n",
    "def get_mask(source, source_lengths):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: [B, C, T]\n",
    "        source_lengths: [B]\n",
    "    Returns:\n",
    "        mask: [B, 1, T]\n",
    "    \"\"\"\n",
    "    B, _, T = source.size()\n",
    "    mask = source.new_ones((B, 1, T))\n",
    "    for i in range(B):\n",
    "        mask[i, :, source_lengths[i]:] = 0\n",
    "    return mask\n",
    "\n",
    "\n",
    "# if __na////h.manual_seed(123)\n",
    "#     B, C, T = 2, 3, 12\n",
    "#     # fake data\n",
    "#     source = torch.randint(4, (B, C, T))\n",
    "#     estimate_source = torch.randint(4, (B, C, T))\n",
    "#     source[1, :, -3:] = 0\n",
    "#     estimate_source[1, :, -3:] = 0\n",
    "#     source_lengths = torch.LongTensor([T, T-3])\n",
    "#     print('source', source)\n",
    "#     print('estimate_source', estimate_source)\n",
    "#     print('source_lengths', source_lengths)\n",
    "    \n",
    "#     loss, max_snr, estimate_source, reorder_estimate_source = cal_loss(source, estimate_source, source_lengths)\n",
    "#     print('loss', loss)\n",
    "#     print('max_snr', max_snr)\n",
    "#     print('reorder_estimate_source', reorder_estimate_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solver.py\n",
    "# Created on 2018/12\n",
    "# Author: Kaituo XU\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "# from pit_criterion import cal_loss\n",
    "\n",
    "\n",
    "class Solver(object):\n",
    "    \n",
    "    def __init__(self, data, model, optimizer, args):\n",
    "        self.tr_loader = data['tr_loader']\n",
    "        self.cv_loader = data['cv_loader']\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Training config\n",
    "        self.use_cuda = args.use_cuda\n",
    "        self.epochs = args.epochs\n",
    "        self.half_lr = args.half_lr\n",
    "        self.early_stop = args.early_stop\n",
    "        self.max_norm = args.max_norm\n",
    "        # save and load model\n",
    "        self.save_folder = args.save_folder\n",
    "        self.checkpoint = args.checkpoint\n",
    "        self.continue_from = args.continue_from\n",
    "        self.model_path = args.model_path\n",
    "        # logging\n",
    "        self.print_freq = args.print_freq\n",
    "        # visualizing loss using visdom\n",
    "        self.tr_loss = torch.Tensor(self.epochs)\n",
    "        self.cv_loss = torch.Tensor(self.epochs)\n",
    "        self.visdom = args.visdom\n",
    "        self.visdom_epoch = args.visdom_epoch\n",
    "        self.visdom_id = args.visdom_id\n",
    "        if self.visdom:\n",
    "            from visdom import Visdom\n",
    "            self.vis = Visdom(env=self.visdom_id)\n",
    "            self.vis_opts = dict(title=self.visdom_id,\n",
    "                                 ylabel='Loss', xlabel='Epoch',\n",
    "                                 legend=['train loss', 'cv loss'])\n",
    "            self.vis_window = None\n",
    "            self.vis_epochs = torch.arange(1, self.epochs + 1)\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        # Reset\n",
    "        if self.continue_from:\n",
    "            print('Loading checkpoint model %s' % self.continue_from)\n",
    "            package = torch.load(self.continue_from)\n",
    "            self.model.module.load_state_dict(package['state_dict'])\n",
    "            self.optimizer.load_state_dict(package['optim_dict'])\n",
    "            self.start_epoch = int(package.get('epoch', 1))\n",
    "            self.tr_loss[:self.start_epoch] = package['tr_loss'][:self.start_epoch]\n",
    "            self.cv_loss[:self.start_epoch] = package['cv_loss'][:self.start_epoch]\n",
    "        else:\n",
    "            self.start_epoch = 0\n",
    "        # Create save folder\n",
    "        os.makedirs(self.save_folder, exist_ok=True)\n",
    "        self.prev_val_loss = float(\"inf\")\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.halving = False\n",
    "        self.val_no_impv = 0\n",
    "\n",
    "    def train(self):\n",
    "        # Train model multi-epoches\n",
    "        for epoch in range(self.start_epoch, self.epochs):\n",
    "            # Train one epoch\n",
    "            print(\"Training...\")\n",
    "            self.model.train()  # Turn on BatchNorm & Dropout\n",
    "            start = time.time()\n",
    "            tr_avg_loss = self._run_one_epoch(epoch)\n",
    "            print('-' * 85)\n",
    "            print('Train Summary | End of Epoch {0} | Time {1:.2f}s | '\n",
    "                  'Train Loss {2:.3f}'.format(\n",
    "                      epoch + 1, time.time() - start, tr_avg_loss))\n",
    "            print('-' * 85)\n",
    "\n",
    "            # Save model each epoch\n",
    "            if self.checkpoint:\n",
    "                file_path = os.path.join(\n",
    "                    self.save_folder, 'epoch%d.pth.tar' % (epoch + 1))\n",
    "                torch.save(self.model.module.serialize(self.model.module,\n",
    "                                                       self.optimizer, epoch + 1,\n",
    "                                                       tr_loss=self.tr_loss,\n",
    "                                                       cv_loss=self.cv_loss),\n",
    "                           file_path)\n",
    "                print('Saving checkpoint model to %s' % file_path)\n",
    "\n",
    "            # Cross validation\n",
    "            print('Cross validation...')\n",
    "            self.model.eval()  # Turn off Batchnorm & Dropout\n",
    "            val_loss = self._run_one_epoch(epoch, cross_valid=True)\n",
    "            print('-' * 85)\n",
    "            print('Valid Summary | End of Epoch {0} | Time {1:.2f}s | '\n",
    "                  'Valid Loss {2:.3f}'.format(\n",
    "                      epoch + 1, time.time() - start, val_loss))\n",
    "            print('-' * 85)\n",
    "\n",
    "            # Adjust learning rate (halving)\n",
    "            if self.half_lr:\n",
    "                if val_loss >= self.prev_val_loss:\n",
    "                    self.val_no_impv += 1\n",
    "                    if self.val_no_impv >= 3:\n",
    "                        self.halving = True\n",
    "                    if self.val_no_impv >= 10 and self.early_stop:\n",
    "                        print(\"No imporvement for 10 epochs, early stopping.\")\n",
    "                        break\n",
    "                else:\n",
    "                    self.val_no_impv = 0\n",
    "            if self.halving:\n",
    "                optim_state = self.optimizer.state_dict()\n",
    "                optim_state['param_groups'][0]['lr'] = \\\n",
    "                    optim_state['param_groups'][0]['lr'] / 2.0\n",
    "                self.optimizer.load_state_dict(optim_state)\n",
    "                print('Learning rate adjusted to: {lr:.6f}'.format(\n",
    "                    lr=optim_state['param_groups'][0]['lr']))\n",
    "                self.halving = False\n",
    "            self.prev_val_loss = val_loss\n",
    "\n",
    "            # Save the best model\n",
    "            self.tr_loss[epoch] = tr_avg_loss\n",
    "            self.cv_loss[epoch] = val_loss\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                file_path = os.path.join(self.save_folder, self.model_path)\n",
    "                torch.save(self.model.module.serialize(self.model.module,\n",
    "                                                       self.optimizer, epoch + 1,\n",
    "                                                       tr_loss=self.tr_loss,\n",
    "                                                       cv_loss=self.cv_loss),\n",
    "                           file_path)\n",
    "                print(\"Find better validated model, saving to %s\" % file_path)\n",
    "\n",
    "            # visualizing loss using visdom\n",
    "            if self.visdom:\n",
    "                x_axis = self.vis_epochs[0:epoch + 1]\n",
    "                y_axis = torch.stack(\n",
    "                    (self.tr_loss[0:epoch + 1], self.cv_loss[0:epoch + 1]), dim=1)\n",
    "                if self.vis_window is None:\n",
    "                    self.vis_window = self.vis.line(\n",
    "                        X=x_axis,\n",
    "                        Y=y_axis,\n",
    "                        opts=self.vis_opts,\n",
    "                    )\n",
    "                else:\n",
    "                    self.vis.line(\n",
    "                        X=x_axis.unsqueeze(0).expand(y_axis.size(\n",
    "                            1), x_axis.size(0)).transpose(0, 1),  # Visdom fix\n",
    "                        Y=y_axis,\n",
    "                        win=self.vis_window,\n",
    "                        update='replace',\n",
    "                    )\n",
    "\n",
    "    def _run_one_epoch(self, epoch, cross_valid=False):\n",
    "        start = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        data_loader = self.tr_loader if not cross_valid else self.cv_loader\n",
    "\n",
    "        # visualizing loss using visdom\n",
    "        if self.visdom_epoch and not cross_valid:\n",
    "            vis_opts_epoch = dict(title=self.visdom_id + \" epoch \" + str(epoch),\n",
    "                                  ylabel='Loss', xlabel='Epoch')\n",
    "            vis_window_epoch = None\n",
    "            vis_iters = torch.arange(1, len(data_loader) + 1)\n",
    "            vis_iters_loss = torch.Tensor(len(data_loader))\n",
    "\n",
    "        for i, (data) in enumerate(data_loader):\n",
    "            padded_mixture, mixture_lengths, padded_source = data\n",
    "            if self.use_cuda:\n",
    "                padded_mixture = padded_mixture.cuda()\n",
    "                mixture_lengths = mixture_lengths.cuda()\n",
    "                padded_source = padded_source.cuda()\n",
    "            estimate_source = self.model(padded_mixture)\n",
    "            loss, max_snr, estimate_source, reorder_estimate_source = \\\n",
    "                cal_loss(padded_source, estimate_source, mixture_lengths)\n",
    "            if not cross_valid:\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(),\n",
    "                                               self.max_norm)\n",
    "                self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if i % self.print_freq == 0:\n",
    "                print('Epoch {0} | Iter {1} | Average Loss {2:.3f} | '\n",
    "                      'Current Loss {3:.6f} | {4:.1f} ms/batch'.format(\n",
    "                          epoch + 1, i + 1, total_loss / (i + 1),\n",
    "                          loss.item(), 1000 * (time.time() - start) / (i + 1)),\n",
    "                      flush=True)\n",
    "\n",
    "            # visualizing loss using visdom\n",
    "            if self.visdom_epoch and not cross_valid:\n",
    "                vis_iters_loss[i] = loss.item()\n",
    "                if i % self.print_freq == 0:\n",
    "                    x_axis = vis_iters[:i+1]\n",
    "                    y_axis = vis_iters_loss[:i+1]\n",
    "                    if vis_window_epoch is None:\n",
    "                        vis_window_epoch = self.vis.line(X=x_axis, Y=y_axis,\n",
    "                                                         opts=vis_opts_epoch)\n",
    "                    else:\n",
    "                        self.vis.line(X=x_axis, Y=y_axis, win=vis_window_epoch,\n",
    "                                      update='replace')\n",
    "\n",
    "        return total_loss / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[2, 4, 2, 0],\n",
      "          [0, 2, 1, 2],\n",
      "          [4, 4, 1, 1]],\n",
      "\n",
      "         [[1, 1, 2, 4],\n",
      "          [4, 1, 3, 0],\n",
      "          [0, 1, 0, 2]]],\n",
      "\n",
      "\n",
      "        [[[4, 2, 1, 1],\n",
      "          [0, 1, 1, 0],\n",
      "          [3, 4, 4, 1]],\n",
      "\n",
      "         [[1, 3, 0, 0],\n",
      "          [4, 1, 1, 2],\n",
      "          [3, 1, 2, 2]]]])\n",
      "tensor([[[2, 4, 2, 2, 5, 6, 1, 1],\n",
      "         [1, 1, 6, 5, 3, 1, 0, 2]],\n",
      "\n",
      "        [[4, 2, 1, 2, 4, 4, 4, 1],\n",
      "         [1, 3, 4, 1, 4, 3, 2, 2]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/speech/f_torch/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "#utils.py\n",
    "# Created on 2018/12\n",
    "# Author: Kaituo XU\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def overlap_and_add(signal, frame_step):\n",
    "    \"\"\"Reconstructs a signal from a framed representation.\n",
    "    Adds potentially overlapping frames of a signal with shape\n",
    "    `[..., frames, frame_length]`, offsetting subsequent frames by `frame_step`.\n",
    "    The resulting tensor has shape `[..., output_size]` where\n",
    "        output_size = (frames - 1) * frame_step + frame_length\n",
    "    Args:\n",
    "        signal: A [..., frames, frame_length] Tensor. All dimensions may be unknown, and rank must be at least 2.\n",
    "        frame_step: An integer denoting overlap offsets. Must be less than or equal to frame_length.\n",
    "    Returns:\n",
    "        A Tensor with shape [..., output_size] containing the overlap-added frames of signal's inner-most two dimensions.\n",
    "        output_size = (frames - 1) * frame_step + frame_length\n",
    "    Based on https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/signal/python/ops/reconstruction_ops.py\n",
    "    \"\"\"\n",
    "    outer_dimensions = signal.size()[:-2]\n",
    "    frames, frame_length = signal.size()[-2:]\n",
    "\n",
    "    subframe_length = math.gcd(frame_length, frame_step)  # gcd=Greatest Common Divisor\n",
    "    subframe_step = frame_step // subframe_length\n",
    "    subframes_per_frame = frame_length // subframe_length\n",
    "    output_size = frame_step * (frames - 1) + frame_length\n",
    "    output_subframes = output_size // subframe_length\n",
    "\n",
    "    subframe_signal = signal.view(*outer_dimensions, -1, subframe_length)\n",
    "\n",
    "    frame = torch.arange(0, output_subframes).unfold(0, subframes_per_frame, subframe_step)\n",
    "    frame = signal.new_tensor(frame).long()  # signal may in GPU or CPU\n",
    "    frame = frame.contiguous().view(-1)\n",
    "\n",
    "    result = signal.new_zeros(*outer_dimensions, output_subframes, subframe_length)\n",
    "    result.index_add_(-2, frame, subframe_signal)\n",
    "    result = result.view(*outer_dimensions, -1)\n",
    "    return result\n",
    "\n",
    "\n",
    "def remove_pad(inputs, inputs_lengths):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        inputs: torch.Tensor, [B, C, T] or [B, T], B is batch size\n",
    "        inputs_lengths: torch.Tensor, [B]\n",
    "    Returns:\n",
    "        results: a list containing B items, each item is [C, T], T varies\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    dim = inputs.dim()\n",
    "    if dim == 3:\n",
    "        C = inputs.size(1)\n",
    "    for input, length in zip(inputs, inputs_lengths):\n",
    "        if dim == 3: # [B, C, T]\n",
    "            results.append(input[:,:length].view(C, -1).cpu().numpy())\n",
    "        elif dim == 2:  # [B, T]\n",
    "            results.append(input[:length].view(-1).cpu().numpy())\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(123)\n",
    "    M, C, K, N = 2, 2, 3, 4\n",
    "    frame_step = 2\n",
    "    signal = torch.randint(5, (M, C, K, N))\n",
    "    result = overlap_and_add(signal, frame_step)\n",
    "    print(signal)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Fully-Convolutional Time-domain Audio Separation Network (Conv-TasNet) with Permutation Invariant Training\n",
      "       [-h] [--train_dir TRAIN_DIR] [--valid_dir VALID_DIR]\n",
      "       [--sample_rate SAMPLE_RATE] [--segment SEGMENT] [--cv_maxlen CV_MAXLEN]\n",
      "       [--N N] [--L L] [--B B] [--H H] [--P P] [--X X] [--R R] [--C C]\n",
      "       [--norm_type {gLN,cLN,BN}] [--causal CAUSAL]\n",
      "       [--mask_nonlinear {relu,softmax}] [--use_cuda USE_CUDA]\n",
      "       [--epochs EPOCHS] [--half_lr HALF_LR] [--early_stop EARLY_STOP]\n",
      "       [--max_norm MAX_NORM] [--shuffle SHUFFLE] [--batch_size BATCH_SIZE]\n",
      "       [--num_workers NUM_WORKERS] [--optimizer {sgd,adam}] [--lr LR]\n",
      "       [--momentum MOMENTUM] [--l2 L2] [--save_folder SAVE_FOLDER]\n",
      "       [--checkpoint CHECKPOINT] [--continue_from CONTINUE_FROM]\n",
      "       [--model_path MODEL_PATH] [--print_freq PRINT_FREQ] [--visdom VISDOM]\n",
      "       [--visdom_epoch VISDOM_EPOCH] [--visdom_id VISDOM_ID]\n",
      "Fully-Convolutional Time-domain Audio Separation Network (Conv-TasNet) with Permutation Invariant Training: error: unrecognized arguments: -f /home/speech/.local/share/jupyter/runtime/kernel-1456dc10-3522-4f36-9ee9-f360d3619c07.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#train.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Created on 2018/12\n",
    "# Author: Kaituo XU\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "\n",
    "# from data import AudioDataLoader, AudioDataset\n",
    "# from solver import Solver\n",
    "# from conv_tasnet import ConvTasNet\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    \"Fully-Convolutional Time-domain Audio Separation Network (Conv-TasNet) \"\n",
    "    \"with Permutation Invariant Training\")\n",
    "# General config\n",
    "# Task related\n",
    "parser.add_argument('--train_dir', type=str, default=None,\n",
    "                    help='directory including mix.json, s1.json and s2.json')\n",
    "parser.add_argument('--valid_dir', type=str, default=None,\n",
    "                    help='directory including mix.json, s1.json and s2.json')\n",
    "parser.add_argument('--sample_rate', default=8000, type=int,\n",
    "                    help='Sample rate')\n",
    "parser.add_argument('--segment', default=4, type=float,\n",
    "                    help='Segment length (seconds)')\n",
    "parser.add_argument('--cv_maxlen', default=8, type=float,\n",
    "                    help='max audio length (seconds) in cv, to avoid OOM issue.')\n",
    "# Network architecture\n",
    "parser.add_argument('--N', default=256, type=int,\n",
    "                    help='Number of filters in autoencoder')\n",
    "parser.add_argument('--L', default=20, type=int,\n",
    "                    help='Length of the filters in samples (40=5ms at 8kHZ)')\n",
    "parser.add_argument('--B', default=256, type=int,\n",
    "                    help='Number of channels in bottleneck 1 Ã— 1-conv block')\n",
    "parser.add_argument('--H', default=512, type=int,\n",
    "                    help='Number of channels in convolutional blocks')\n",
    "parser.add_argument('--P', default=3, type=int,\n",
    "                    help='Kernel size in convolutional blocks')\n",
    "parser.add_argument('--X', default=8, type=int,\n",
    "                    help='Number of convolutional blocks in each repeat')\n",
    "parser.add_argument('--R', default=4, type=int,\n",
    "                    help='Number of repeats')\n",
    "parser.add_argument('--C', default=2, type=int,\n",
    "                    help='Number of speakers')\n",
    "parser.add_argument('--norm_type', default='gLN', type=str,\n",
    "                    choices=['gLN', 'cLN', 'BN'], help='Layer norm type')\n",
    "parser.add_argument('--causal', type=int, default=0,\n",
    "                    help='Causal (1) or noncausal(0) training')\n",
    "parser.add_argument('--mask_nonlinear', default='relu', type=str,\n",
    "                    choices=['relu', 'softmax'], help='non-linear to generate mask')\n",
    "# Training config\n",
    "parser.add_argument('--use_cuda', type=int, default=1,\n",
    "                    help='Whether use GPU')\n",
    "parser.add_argument('--epochs', default=30, type=int,\n",
    "                    help='Number of maximum epochs')\n",
    "parser.add_argument('--half_lr', dest='half_lr', default=0, type=int,\n",
    "                    help='Halving learning rate when get small improvement')\n",
    "parser.add_argument('--early_stop', dest='early_stop', default=0, type=int,\n",
    "                    help='Early stop training when no improvement for 10 epochs')\n",
    "parser.add_argument('--max_norm', default=5, type=float,\n",
    "                    help='Gradient norm threshold to clip')\n",
    "# minibatch\n",
    "parser.add_argument('--shuffle', default=0, type=int,\n",
    "                    help='reshuffle the data at every epoch')\n",
    "parser.add_argument('--batch_size', default=128, type=int,\n",
    "                    help='Batch size')\n",
    "parser.add_argument('--num_workers', default=4, type=int,\n",
    "                    help='Number of workers to generate minibatch')\n",
    "# optimizer\n",
    "parser.add_argument('--optimizer', default='adam', type=str,\n",
    "                    choices=['sgd', 'adam'],\n",
    "                    help='Optimizer (support sgd and adam now)')\n",
    "parser.add_argument('--lr', default=1e-3, type=float,\n",
    "                    help='Init learning rate')\n",
    "parser.add_argument('--momentum', default=0.0, type=float,\n",
    "                    help='Momentum for optimizer')\n",
    "parser.add_argument('--l2', default=0.0, type=float,\n",
    "                    help='weight decay (L2 penalty)')\n",
    "# save and load model\n",
    "parser.add_argument('--save_folder', default='exp/temp',\n",
    "                    help='Location to save epoch models')\n",
    "parser.add_argument('--checkpoint', dest='checkpoint', default=0, type=int,\n",
    "                    help='Enables checkpoint saving of model')\n",
    "parser.add_argument('--continue_from', default='',\n",
    "                    help='Continue from checkpoint model')\n",
    "parser.add_argument('--model_path', default='final.pth.tar',\n",
    "                    help='Location to save best validation model')\n",
    "# logging\n",
    "parser.add_argument('--print_freq', default=10, type=int,\n",
    "                    help='Frequency of printing training infomation')\n",
    "parser.add_argument('--visdom', dest='visdom', type=int, default=0,\n",
    "                    help='Turn on visdom graphing')\n",
    "parser.add_argument('--visdom_epoch', dest='visdom_epoch', type=int, default=0,\n",
    "                    help='Turn on visdom graphing each epoch')\n",
    "parser.add_argument('--visdom_id', default='TasNet training',\n",
    "                    help='Identifier for visdom run')\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Construct Solver\n",
    "    # data\n",
    "    tr_dataset = AudioDataset(args.train_dir, args.batch_size,\n",
    "                              sample_rate=args.sample_rate, segment=args.segment)\n",
    "    cv_dataset = AudioDataset(args.valid_dir, batch_size=1,  # 1 -> use less GPU memory to do cv\n",
    "                              sample_rate=args.sample_rate,\n",
    "                              segment=-1, cv_maxlen=args.cv_maxlen)  # -1 -> use full audio\n",
    "    tr_loader = AudioDataLoader(tr_dataset, batch_size=1,\n",
    "                                shuffle=args.shuffle,\n",
    "                                num_workers=args.num_workers)\n",
    "    cv_loader = AudioDataLoader(cv_dataset, batch_size=1,\n",
    "                                num_workers=0)\n",
    "    data = {'tr_loader': tr_loader, 'cv_loader': cv_loader}\n",
    "    # model\n",
    "    model = ConvTasNet(args.N, args.L, args.B, args.H, args.P, args.X, args.R,\n",
    "                       args.C, norm_type=args.norm_type, causal=args.causal,\n",
    "                       mask_nonlinear=args.mask_nonlinear)\n",
    "    print(model)\n",
    "    if args.use_cuda:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        model.cuda()\n",
    "    # optimizer\n",
    "    if args.optimizer == 'sgd':\n",
    "        optimizier = torch.optim.SGD(model.parameters(),\n",
    "                                     lr=args.lr,\n",
    "                                     momentum=args.momentum,\n",
    "                                     weight_decay=args.l2)\n",
    "    elif args.optimizer == 'adam':\n",
    "        optimizier = torch.optim.Adam(model.parameters(),\n",
    "                                      lr=args.lr,\n",
    "                                      weight_decay=args.l2)\n",
    "    else:\n",
    "        print(\"Not support optimizer\")\n",
    "        return\n",
    "\n",
    "    # solver\n",
    "    solver = Solver(data, model, optimizier, args)\n",
    "    solver.train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    main(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f_torch",
   "language": "python",
   "name": "f_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
