{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess.py\n",
    "import json\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "\n",
    "\n",
    "def preprocess_one_dir(in_dir, out_dir, out_filename, sample_rate=16000):\n",
    "    file_infos = []\n",
    "    in_dir = os.path.abspath(in_dir)\n",
    "    wav_list = os.listdir(in_dir)\n",
    "    for wav_file in wav_list:\n",
    "        if not wav_file.endswith('.wav'):\n",
    "            continue\n",
    "        wav_path = os.path.join(in_dir, wav_file)\n",
    "        samples, _ = librosa.load(wav_path, sr=sample_rate)\n",
    "        file_infos.append((wav_path, len(samples)))\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    with open(os.path.join(out_dir, out_filename + '.json'), 'w') as f:\n",
    "        json.dump(file_infos, f, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#fatemeh#def preprocess(args):\n",
    "  #fatemeh#  for data_type in ['tr', 'cv', 'tt']:\n",
    "  #fatemeh#      for speaker in ['mix', 's1', 's2']:\n",
    "  #fatemeh#          preprocess_one_dir(os.path.join(args.in_dir, data_type, speaker),\n",
    "  #fatemeh#                             os.path.join(args.out_dir, data_type),\n",
    "  #fatemeh#                             speaker,\n",
    "  #fatemeh#                             sample_rate=args.sample_rate)\n",
    "            \n",
    "#fatemeh delete args\n",
    "def preprocess(in_dir,out_dir,sample_rate):\n",
    "    for data_type in ['tr', 'cv', 'tt']:\n",
    "        for speaker in ['mix', 's1', 's2']:\n",
    "            preprocess_one_dir(os.path.join(in_dir, data_type, speaker),\n",
    "                               os.path.join(out_dir, data_type),\n",
    "                               speaker,\n",
    "                               sample_rate=sample_rate)\n",
    "#%%\n",
    "if __name__ == \"__main__\":\n",
    "    in_dir=\"/home/speech/f_torch/bin/stream_data/data\"\n",
    "\n",
    "    out_dir=\"/home/speech/f_torch/bin/stream_data/outdata\"\n",
    "    sample_rate=16000\n",
    "    preprocess(in_dir,out_dir,sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[2., 4., 2., 0.],\n",
      "          [0., 2., 1., 2.],\n",
      "          [4., 4., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 2., 4.],\n",
      "          [4., 1., 3., 0.],\n",
      "          [0., 1., 0., 2.]]],\n",
      "\n",
      "\n",
      "        [[[4., 2., 1., 1.],\n",
      "          [0., 1., 1., 0.],\n",
      "          [3., 4., 4., 1.]],\n",
      "\n",
      "         [[1., 3., 0., 0.],\n",
      "          [4., 1., 1., 2.],\n",
      "          [3., 1., 2., 2.]]]])\n",
      "tensor([[[2., 4., 2., 2., 5., 6., 1., 1.],\n",
      "         [1., 1., 6., 5., 3., 1., 0., 2.]],\n",
      "\n",
      "        [[4., 2., 1., 2., 4., 4., 4., 1.],\n",
      "         [1., 3., 4., 1., 4., 3., 2., 2.]]])\n"
     ]
    }
   ],
   "source": [
    "#utils.py\n",
    "# Created on 2018/12\n",
    "# Author: Kaituo XU\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def overlap_and_add(signal, frame_step):\n",
    "    \"\"\"Reconstructs a signal from a framed representation.\n",
    "\n",
    "    Adds potentially overlapping frames of a signal with shape\n",
    "    `[..., frames, frame_length]`, offsetting subsequent frames by `frame_step`.\n",
    "    The resulting tensor has shape `[..., output_size]` where\n",
    "\n",
    "        output_size = (frames - 1) * frame_step + frame_length\n",
    "\n",
    "    Args:\n",
    "        signal: A [..., frames, frame_length] Tensor. All dimensions may be unknown, and rank must be at least 2.\n",
    "        frame_step: An integer denoting overlap offsets. Must be less than or equal to frame_length.\n",
    "\n",
    "    Returns:\n",
    "        A Tensor with shape [..., output_size] containing the overlap-added frames of signal's inner-most two dimensions.\n",
    "        output_size = (frames - 1) * frame_step + frame_length\n",
    "\n",
    "    Based on https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/contrib/signal/python/ops/reconstruction_ops.py\n",
    "    \"\"\"\n",
    "    outer_dimensions = signal.size()[:-2]\n",
    "    frames, frame_length = signal.size()[-2:]\n",
    "\n",
    "    subframe_length = math.gcd(frame_length, frame_step)  # gcd=Greatest Common Divisor\n",
    "    subframe_step = frame_step // subframe_length\n",
    "    subframes_per_frame = frame_length // subframe_length\n",
    "    output_size = frame_step * (frames - 1) + frame_length\n",
    "    output_subframes = output_size // subframe_length\n",
    "\n",
    "    subframe_signal = signal.view(*outer_dimensions, -1, subframe_length)\n",
    "\n",
    "    frame = torch.arange(0, output_subframes).unfold(0, subframes_per_frame, subframe_step)\n",
    "    frame = signal.new_tensor(frame).long()  # signal may in GPU or CPU\n",
    "    frame = frame.contiguous().view(-1)\n",
    "\n",
    "    result = signal.new_zeros(*outer_dimensions, output_subframes, subframe_length)\n",
    "    result.index_add_(-2, frame, subframe_signal)\n",
    "    result = result.view(*outer_dimensions, -1)\n",
    "    return result\n",
    "\n",
    "\n",
    "def remove_pad(inputs, inputs_lengths):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        inputs: torch.Tensor, [B, C, T] or [B, T], B is batch size\n",
    "        inputs_lengths: torch.Tensor, [B]\n",
    "    Returns:\n",
    "        results: a list containing B items, each item is [C, T], T varies\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    dim = inputs.dim()\n",
    "    if dim == 3:\n",
    "        C = inputs.size(1)\n",
    "    for input, length in zip(inputs, inputs_lengths):\n",
    "        if dim == 3: # [B, C, T]\n",
    "            results.append(input[:,:length].view(C, -1).cpu().numpy())\n",
    "        elif dim == 2:  # [B, T]\n",
    "            results.append(input[:length].view(-1).cpu().numpy())\n",
    "    return results\n",
    "\n",
    "#%%\n",
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(123)\n",
    "    M, C, K, N = 2, 2, 3, 4\n",
    "    frame_step = 2\n",
    "    signal = torch.randint(5, (M, C, K, N))\n",
    "    result = overlap_and_add(signal, frame_step)\n",
    "    print(signal)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pit_criterion.py\n",
    "# Created on 2018/12\n",
    "# Author: Kaituo XU\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "\n",
    "def cal_loss(source, estimate_source, source_lengths):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: [B, C, T], B is batch size\n",
    "        estimate_source: [B, C, T]\n",
    "        source_lengths: [B]\n",
    "    \"\"\"\n",
    "    max_snr, perms, max_snr_idx = cal_si_snr_with_pit(source,\n",
    "                                                      estimate_source,\n",
    "                                                      source_lengths)\n",
    "    loss = 0 - torch.mean(max_snr)\n",
    "    reorder_estimate_source = reorder_source(estimate_source, perms, max_snr_idx)\n",
    "    return loss, max_snr, estimate_source, reorder_estimate_source\n",
    "\n",
    "\n",
    "def cal_si_snr_with_pit(source, estimate_source, source_lengths):\n",
    "    \"\"\"Calculate SI-SNR with PIT training.\n",
    "    Args:\n",
    "        source: [B, C, T], B is batch size\n",
    "        estimate_source: [B, C, T]\n",
    "        source_lengths: [B], each item is between [0, T]\n",
    "    \"\"\"\n",
    "    assert source.size() == estimate_source.size()\n",
    "    B, C, T = source.size()\n",
    "    # mask padding position along T\n",
    "    mask = get_mask(source, source_lengths)\n",
    "    estimate_source *= mask\n",
    "\n",
    "    # Step 1. Zero-mean norm\n",
    "    num_samples = source_lengths.view(-1, 1, 1).float()  # [B, 1, 1]\n",
    "    mean_target = torch.sum(source, dim=2, keepdim=True) / num_samples\n",
    "    mean_estimate = torch.sum(estimate_source, dim=2, keepdim=True) / num_samples\n",
    "    zero_mean_target = source - mean_target\n",
    "    zero_mean_estimate = estimate_source - mean_estimate\n",
    "    # mask padding position along T\n",
    "    zero_mean_target *= mask\n",
    "    zero_mean_estimate *= mask\n",
    "\n",
    "    # Step 2. SI-SNR with PIT\n",
    "    # reshape to use broadcast\n",
    "    s_target = torch.unsqueeze(zero_mean_target, dim=1)  # [B, 1, C, T]\n",
    "    s_estimate = torch.unsqueeze(zero_mean_estimate, dim=2)  # [B, C, 1, T]\n",
    "    # s_target = <s', s>s / ||s||^2\n",
    "    pair_wise_dot = torch.sum(s_estimate * s_target, dim=3, keepdim=True)  # [B, C, C, 1]\n",
    "    s_target_energy = torch.sum(s_target ** 2, dim=3, keepdim=True) + EPS  # [B, 1, C, 1]\n",
    "    pair_wise_proj = pair_wise_dot * s_target / s_target_energy  # [B, C, C, T]\n",
    "    # e_noise = s' - s_target\n",
    "    e_noise = s_estimate - pair_wise_proj  # [B, C, C, T]\n",
    "    # SI-SNR = 10 * log_10(||s_target||^2 / ||e_noise||^2)\n",
    "    pair_wise_si_snr = torch.sum(pair_wise_proj ** 2, dim=3) / (torch.sum(e_noise ** 2, dim=3) + EPS)\n",
    "    pair_wise_si_snr = 10 * torch.log10(pair_wise_si_snr + EPS)  # [B, C, C]\n",
    "\n",
    "    # Get max_snr of each utterance\n",
    "    # permutations, [C!, C]\n",
    "    perms = source.new_tensor(list(permutations(range(C))), dtype=torch.long)\n",
    "    # one-hot, [C!, C, C]\n",
    "    index = torch.unsqueeze(perms, 2)\n",
    "    perms_one_hot = source.new_zeros((*perms.size(), C)).scatter_(2, index, 1)\n",
    "    # [B, C!] <- [B, C, C] einsum [C!, C, C], SI-SNR sum of each permutation\n",
    "    snr_set = torch.einsum('bij,pij->bp', [pair_wise_si_snr, perms_one_hot])\n",
    "    max_snr_idx = torch.argmax(snr_set, dim=1)  # [B]\n",
    "    # max_snr = torch.gather(snr_set, 1, max_snr_idx.view(-1, 1))  # [B, 1]\n",
    "    max_snr, _ = torch.max(snr_set, dim=1, keepdim=True)\n",
    "    max_snr /= C\n",
    "    return max_snr, perms, max_snr_idx\n",
    "\n",
    "\n",
    "def reorder_source(source, perms, max_snr_idx):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: [B, C, T]\n",
    "        perms: [C!, C], permutations\n",
    "        max_snr_idx: [B], each item is between [0, C!)\n",
    "    Returns:\n",
    "        reorder_source: [B, C, T]\n",
    "    \"\"\"\n",
    "    B, C, *_ = source.size()\n",
    "    # [B, C], permutation whose SI-SNR is max of each utterance\n",
    "    # for each utterance, reorder estimate source according this permutation\n",
    "    max_snr_perm = torch.index_select(perms, dim=0, index=max_snr_idx)\n",
    "    # print('max_snr_perm', max_snr_perm)\n",
    "    # maybe use torch.gather()/index_select()/scatter() to impl this?\n",
    "    reorder_source = torch.zeros_like(source)\n",
    "    for b in range(B):\n",
    "        for c in range(C):\n",
    "            reorder_source[b, c] = source[b, max_snr_perm[b][c]]\n",
    "    return reorder_source\n",
    "\n",
    "\n",
    "def get_mask(source, source_lengths):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: [B, C, T]\n",
    "        source_lengths: [B]\n",
    "    Returns:\n",
    "        mask: [B, 1, T]\n",
    "    \"\"\"\n",
    "    B, _, T = source.size()\n",
    "    mask = source.new_ones((B, 1, T))\n",
    "    for i in range(B):\n",
    "        mask[i, :, source_lengths[i]:] = 0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.py:\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "import librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "json_dir='/home/speech/f_torch/bin/outdata/tr'\n",
    "mix_json = os.path.join(json_dir, 'mix.json')\n",
    "s1_json = os.path.join(json_dir, 's1.json')\n",
    "s2_json = os.path.join(json_dir, 's2.json')\n",
    "with open(mix_json, 'r') as f:\n",
    "    mix_infos = json.load(f)\n",
    "with open(s1_json, 'r') as f:\n",
    "    s1_infos = json.load(f)\n",
    "with open(s2_json, 'r') as f:\n",
    "    s2_infos = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['/home/speech/f_torch/bin/data/tr/mix/mix45_fm.wav', 67175],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix15_fm.wav', 46797],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix35_fm.wav', 57959],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix17_fm.wav', 46797],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix38_fm.wav', 36148],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix18_fm.wav', 36148],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix24_ff.wav', 42804],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix13_ff.wav', 46797],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix14_ff.wav', 46797],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix37_fm.wav', 47002],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix25_fm.wav', 42804],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix28_fm.wav', 36148],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix26_fm.wav', 36967],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix23_ff.wav', 42804],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix27_fm.wav', 42804],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix12_ff.wav', 42804],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix16_fm.wav', 36967],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix36_fm.wav', 36967],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix34_ff.wav', 57959],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix46_fm.wav', 36967]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_infos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort(infos):\n",
    "    return sorted(infos, key=lambda info: int(info[1]), reverse=True)\n",
    "sorted_mix_infos = sort(mix_infos)\n",
    "sorted_s1_infos = sort(s1_infos)\n",
    "sorted_s2_infos = sort(s2_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/speech/f_torch/bin/data/tr/mix/mix15_fm.wav', 46797]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_infos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['/home/speech/f_torch/bin/data/tr/mix/mix45_fm.wav', 67175],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix35_fm.wav', 57959],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix34_ff.wav', 57959],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix37_fm.wav', 47002],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix15_fm.wav', 46797],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix17_fm.wav', 46797],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix13_ff.wav', 46797],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix14_ff.wav', 46797],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix24_ff.wav', 42804],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix25_fm.wav', 42804],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix23_ff.wav', 42804],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix27_fm.wav', 42804],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix12_ff.wav', 42804],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix26_fm.wav', 36967],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix16_fm.wav', 36967],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix36_fm.wav', 36967],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix46_fm.wav', 36967],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix38_fm.wav', 36148],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix18_fm.wav', 36148],\n",
       " ['/home/speech/f_torch/bin/data/tr/mix/mix28_fm.wav', 36148]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_mix_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop 0 utts(0.00 h) which is short than 16000 samples\n"
     ]
    }
   ],
   "source": [
    "# segment length and count dropped utts\n",
    "segment=1\n",
    "if segment >= 0.0:\n",
    "            # segment length and count dropped utts\n",
    "            segment_len = int(segment * sample_rate)  # 4s * 8000/s = 32000 samples\n",
    "            drop_utt, drop_len = 0, 0\n",
    "            for _, sample in sorted_mix_infos:\n",
    "                if sample < segment_len:\n",
    "                    drop_utt += 1\n",
    "                    drop_len += sample\n",
    "            print(\"Drop {} utts({:.2f} h) which is short than {} samples\".format(\n",
    "                drop_utt, drop_len/sample_rate/36000, segment_len))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len(sorted_mix_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1\n",
    "# generate minibach infomations\n",
    "minibatch = []\n",
    "start = 0\n",
    "while True:\n",
    "    num_segments = 0\n",
    "    end = start\n",
    "    part_mix, part_s1, part_s2 = [], [], []\n",
    "    while num_segments < batch_size and end < len(sorted_mix_infos):\n",
    "        utt_len = int(sorted_mix_infos[end][1])\n",
    "        if utt_len >= segment_len:  # skip too short utt\n",
    "            num_segments += math.ceil(utt_len / segment_len)\n",
    "            # Ensure num_segments is less than batch_size\n",
    "            if num_segments > batch_size:\n",
    "                # if num_segments of 1st audio > batch_size, skip it\n",
    "                if start == end: end += 1\n",
    "                break\n",
    "            part_mix.append(sorted_mix_infos[end])\n",
    "            part_s1.append(sorted_s1_infos[end])\n",
    "            part_s2.append(sorted_s2_infos[end])\n",
    "        end += 1\n",
    "    if len(part_mix) > 0:\n",
    "        minibatch.append([part_mix, part_s1, part_s2,sample_rate, segment_len])\n",
    "    if end == len(sorted_mix_infos):\n",
    "        break\n",
    "    start = end\n",
    "self_minibatch = minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full utterance but not segment\n",
    "# generate minibach infomations\n",
    "segment=-1\n",
    "batch_size=2\n",
    "cv_maxlen=8\n",
    "minibatch = []\n",
    "start = 0\n",
    "while True:\n",
    "    end = min(len(sorted_mix_infos), start + batch_size)\n",
    "    # Skip long audio to avoid out-of-memory issue\n",
    "    if int(sorted_mix_infos[start][1]) > cv_maxlen * sample_rate:\n",
    "        start = end\n",
    "        continue\n",
    "    minibatch.append([sorted_mix_infos[start:end],sorted_s1_infos[start:end], sorted_s2_infos[start:end], sample_rate, segment])\n",
    "    if end == len(sorted_mix_infos):\n",
    "        break\n",
    "    start = end\n",
    "self_minibatch = minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[['/home/speech/f_torch/bin/data/tr/mix/mix45_fm.wav', 67175],\n",
       "   ['/home/speech/f_torch/bin/data/tr/mix/mix35_fm.wav', 57959]],\n",
       "  [],\n",
       "  [],\n",
       "  16000,\n",
       "  -1],\n",
       " [[['/home/speech/f_torch/bin/data/tr/mix/mix34_ff.wav', 57959],\n",
       "   ['/home/speech/f_torch/bin/data/tr/mix/mix37_fm.wav', 47002]],\n",
       "  [],\n",
       "  [],\n",
       "  16000,\n",
       "  -1],\n",
       " [[['/home/speech/f_torch/bin/data/tr/mix/mix15_fm.wav', 46797],\n",
       "   ['/home/speech/f_torch/bin/data/tr/mix/mix17_fm.wav', 46797]],\n",
       "  [],\n",
       "  [],\n",
       "  16000,\n",
       "  -1],\n",
       " [[['/home/speech/f_torch/bin/data/tr/mix/mix13_ff.wav', 46797],\n",
       "   ['/home/speech/f_torch/bin/data/tr/mix/mix14_ff.wav', 46797]],\n",
       "  [],\n",
       "  [],\n",
       "  16000,\n",
       "  -1],\n",
       " [[['/home/speech/f_torch/bin/data/tr/mix/mix24_ff.wav', 42804],\n",
       "   ['/home/speech/f_torch/bin/data/tr/mix/mix25_fm.wav', 42804]],\n",
       "  [],\n",
       "  [],\n",
       "  16000,\n",
       "  -1],\n",
       " [[['/home/speech/f_torch/bin/data/tr/mix/mix23_ff.wav', 42804],\n",
       "   ['/home/speech/f_torch/bin/data/tr/mix/mix27_fm.wav', 42804]],\n",
       "  [],\n",
       "  [],\n",
       "  16000,\n",
       "  -1],\n",
       " [[['/home/speech/f_torch/bin/data/tr/mix/mix12_ff.wav', 42804],\n",
       "   ['/home/speech/f_torch/bin/data/tr/mix/mix26_fm.wav', 36967]],\n",
       "  [],\n",
       "  [],\n",
       "  16000,\n",
       "  -1],\n",
       " [[['/home/speech/f_torch/bin/data/tr/mix/mix16_fm.wav', 36967],\n",
       "   ['/home/speech/f_torch/bin/data/tr/mix/mix36_fm.wav', 36967]],\n",
       "  [],\n",
       "  [],\n",
       "  16000,\n",
       "  -1],\n",
       " [[['/home/speech/f_torch/bin/data/tr/mix/mix46_fm.wav', 36967],\n",
       "   ['/home/speech/f_torch/bin/data/tr/mix/mix38_fm.wav', 36148]],\n",
       "  [],\n",
       "  [],\n",
       "  16000,\n",
       "  -1],\n",
       " [[['/home/speech/f_torch/bin/data/tr/mix/mix18_fm.wav', 36148],\n",
       "   ['/home/speech/f_torch/bin/data/tr/mix/mix28_fm.wav', 36148]],\n",
       "  [],\n",
       "  [],\n",
       "  16000,\n",
       "  -1]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(minibatch))\n",
    "minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f_torch",
   "language": "python",
   "name": "f_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
